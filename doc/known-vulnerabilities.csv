DependencyName,DependencyPath,Description,License,Md5,Sha1,Identifiers,CPE,CVE,CWE,Vulnerability,Source,CVSSv2_Severity,CVSSv2_Score,CVSSv2,CVSSv3_BaseSeverity,CVSSv3_BaseScore,CVSSv3,CPE Confidence,Evidence Count,VendorProject,Product,Name,DateAdded,ShortDescription,RequiredAction,DueDate,Notes
commons-compress-1.22.jar,/home/runner/.m2/repository/org/apache/commons/commons-compress/1.22/commons-compress-1.22.jar,"Apache Commons Compress software defines an API for working with compression and archive formats.  These include: bzip2, gzip, pack200, lzma, xz, Snappy, traditional Unix Compress, DEFLATE, DEFLATE64, LZ4, Brotli, Zstandard and ar, cpio, jar, tar, zip, dump, 7z, arj.",https://www.apache.org/licenses/LICENSE-2.0.txt,f1e4db16fee4291212d91409313a8086,691a8b4e6cf4248c3bc72c8b719337d5cb7359fa,pkg:maven/org.apache.commons/commons-compress@1.22,cpe:2.3:a:apache:commons_compress:1.22:*:*:*:*:*:*:*,CVE-2023-42503,"CWE-400 Uncontrolled Resource Consumption, CWE-20 Improper Input Validation","Improper Input Validation, Uncontrolled Resource Consumption vulnerability in Apache Commons Compress in TAR parsing.This issue affects Apache Commons Compress: from 1.22 before 1.24.0.  Users are recommended to upgrade to version 1.24.0, which fixes the issue.  A third party can create a malformed TAR file by manipulating file modification times headers, which when parsed with Apache Commons Compress, will cause a denial of service issue via CPU consumption.  In version 1.22 of Apache Commons Compress, support was added for file modification times with higher precision (issue # COMPRESS-612 [1]). The format for the PAX extended headers carrying this data consists of two numbers separated by a period [2], indicating seconds and subsecond precision (for example “1647221103.5998539”). The impacted fields are “atime”, “ctime”, “mtime” and “LIBARCHIVE.creationtime”. No input validation is performed prior to the parsing of header values.  Parsing of these numbers uses the BigDecimal [3] class from the JDK which has a publicly known algorithmic complexity issue when doing operations on large numbers, causing denial of service (see issue # JDK-6560193 [4]). A third party can manipulate file time headers in a TAR file by placing a number with a very long fraction (300,000 digits) or a number with exponent notation (such as “9e9999999”) within a file modification time header, and the parsing of files with these headers will take hours instead of seconds, leading to a denial of service via exhaustion of CPU resources. This issue is similar to CVE-2012-2098 [5].  [1]:  https://issues.apache.org/jira/browse/COMPRESS-612  [2]:  https://pubs.opengroup.org/onlinepubs/9699919799/utilities/pax.html#tag_20_92_13_05  [3]:  https://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html  [4]:  https://bugs.openjdk.org/browse/JDK-6560193  [5]:  https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2012-2098   Only applications using CompressorStreamFactory class (with auto-detection of file types), TarArchiveInputStream and TarFile classes to parse TAR files are impacted. Since this code was introduced in v1.22, only that version and later versions are impacted.",NVD,,,,MEDIUM,5.5,CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H,HIGH,106,,,,,,,,
jackson-databind-2.14.2.jar,/home/runner/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.14.2/jackson-databind-2.14.2.jar,General data-binding functionality for Jackson: works on core streaming API,"The Apache Software License, Version 2.0: https://www.apache.org/licenses/LICENSE-2.0.txt",c1b12dd14734cd1986132bf55042dd7e,01e71fddbc80bb86f71a6345ac1e8ab8a00e7134,pkg:maven/com.fasterxml.jackson.core/jackson-databind@2.14.2,"cpe:2.3:a:fasterxml:jackson-databind:2.14.2:*:*:*:*:*:*:*, cpe:2.3:a:fasterxml:jackson-modules-java8:2.14.2:*:*:*:*:*:*:*",CVE-2023-35116,CWE-770 Allocation of Resources Without Limits or Throttling,"jackson-databind through 2.15.2 allows attackers to cause a denial of service or other unspecified impact via a crafted object that uses cyclic dependencies. NOTE: the vendor's perspective is that this is not a valid vulnerability report, because the steps of constructing a cyclic data structure and trying to serialize it cannot be achieved by an external attacker.",NVD,,,,MEDIUM,4.7,CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H,HIGH,40,,,,,,,,
